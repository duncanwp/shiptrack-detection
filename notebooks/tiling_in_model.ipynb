{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "316c884a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n",
      "channels_last\n"
     ]
    }
   ],
   "source": [
    "from shiptrack import get_data, get_preprocessing, losses, fit_model\n",
    "from segmentation_models import get_preprocessing\n",
    "from segmentation_models import Unet\n",
    "from segmentation_models.metrics import iou_score\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcb62ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "epochs = 30\n",
    "batch_size = 8\n",
    "learning_rate = 0.01\n",
    "augment = False\n",
    "encoder_freeze = False\n",
    "backbone = \"resnet152\"\n",
    "test_prop = 5\n",
    "loss = \"bce_jaccard_loss\"\n",
    "\n",
    "INT_IMG_SIZE = (2240, 1344)\n",
    "IMG_SIZE = 448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf384cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "model_name = f\"{now.strftime(\"%Y%m%d_%H%M%S\")}_new_{backbone}_{loss}{'_augmented' if augment else ''}\"\n",
    "# System paths\n",
    "training_dir = \"/lustre_scratch/duncanwp/combined_v3_typed_new_composite\"\n",
    "tensorboard_dir = f\"/lustre_scratch/duncanwp/tensorboard/{model_name}\"\n",
    "model_dir = f\"/lustre_scratch/duncanwp/models/{model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c534f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(all_data, all_labels):\n",
    "\n",
    "    for data, labels in zip(all_data, all_labels):\n",
    "#         print(data)\n",
    "        # Resize the data\n",
    "        _data = tf.image.resize(data, INT_IMG_SIZE) / 255.\n",
    "        _labels = tf.image.resize(tf.expand_dims(labels, -1), INT_IMG_SIZE, 'nearest') # Adding an extra color dim for tf.image\n",
    "#         print(_data)\n",
    "#         print(_labels)\n",
    "\n",
    "        # Slice the images to the final size...\n",
    "        flat_patches = tf.image.extract_patches(images=tf.expand_dims(_data, axis=0),\n",
    "                                                sizes=[1, IMG_SIZE, IMG_SIZE, 1],\n",
    "                                                strides=[1, IMG_SIZE, IMG_SIZE, 1],  # This should be the same as sizes\n",
    "                                                rates=[1, 1, 1, 1],\n",
    "                                                padding='VALID')\n",
    "        _data = tf.reshape(flat_patches, [-1, IMG_SIZE, IMG_SIZE, 3])  # Stack them along the leading dim\n",
    "\n",
    "        # ...And the labels\n",
    "        flat_patches = tf.image.extract_patches(images=tf.expand_dims(_labels, axis=0),\n",
    "                                                sizes=[1, IMG_SIZE, IMG_SIZE, 1],\n",
    "                                                strides=[1, IMG_SIZE, IMG_SIZE, 1],  # This should be the same as sizes\n",
    "                                                rates=[1, 1, 1, 1],\n",
    "                                                padding='VALID')\n",
    "        _labels = tf.reshape(flat_patches, [-1, IMG_SIZE, IMG_SIZE])  # Stack them along the leading dim\n",
    "#         print(\"done slicing\")\n",
    "\n",
    "        has_labels = tf.math.reduce_any(tf.reshape(_labels, [-1, IMG_SIZE*IMG_SIZE]) > 0, axis=1)\n",
    "#         print(has_labels)\n",
    "        _data = tf.boolean_mask(_data, has_labels)\n",
    "        _labels = tf.boolean_mask(_labels, has_labels)\n",
    "        for i in range(_data.shape[0]):\n",
    "            yield _data[i], _labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c27cc224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 test samples\n",
      "380 val samples\n",
      "1715 train samples\n"
     ]
    }
   ],
   "source": [
    "from shiptrack import load_numpy_arrays\n",
    "all_data, all_labels = load_numpy_arrays(training_dir)\n",
    "\n",
    "n_test = (all_data.shape[0] // 100) * test_prop\n",
    "n_val = int((all_data.shape[0]-n_test)*0.181818)  # Fixed validation proportion of ~15% of original dataset\n",
    "n_train = all_data.shape[0]-n_test-n_val\n",
    "\n",
    "#     x_test, x_val, x_train = np.split(all_data, [n_test, n_test+n_val])\n",
    "#     y_test, y_val, y_train = np.split(all_labels, [n_test, n_test+n_val])\n",
    "\n",
    "#     np.split seems to be reading all the data into memory\n",
    "\n",
    "x_test, x_val, x_train = all_data[:n_test], all_data[n_test:n_test+n_val], all_data[n_test+n_val:]\n",
    "y_test, y_val, y_train = all_labels[:n_test], all_labels[n_test:n_test+n_val], all_labels[n_test+n_val:]\n",
    "\n",
    "print(n_test, 'test samples')\n",
    "print(n_val, 'val samples')\n",
    "print(n_train, 'train samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d7b9f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labels need to be floats for comparison with the model output\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train.astype('float32')))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val.astype('float32')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68cd7b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _data = tf.image.resize(data, INT_IMG_SIZE) / 255.\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def tile(data):\n",
    "    # Slice the images to the final size...\n",
    "#     flat_patches = tf.image.extract_patches(images=tf.expand_dims(data, axis=0),\n",
    "    flat_patches = tf.image.extract_patches(images=data,            \n",
    "                                            sizes=[1, IMG_SIZE, IMG_SIZE, 1],\n",
    "                                            strides=[1, IMG_SIZE, IMG_SIZE, 1],  # This should be the same as sizes\n",
    "                                            rates=[1, 1, 1, 1],\n",
    "                                            padding='VALID')\n",
    "    _data = tf.reshape(flat_patches, [-1, IMG_SIZE, IMG_SIZE, 3])  # Stack them along the leading dim\n",
    "    return _data\n",
    "\n",
    "\n",
    "\n",
    "resize_and_rescale = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Resizing(*INT_IMG_SIZE),\n",
    "  layers.experimental.preprocessing.Rescaling(1./255)\n",
    "])\n",
    "\n",
    "rescale = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Rescaling(1./255)\n",
    "])\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "ORIG_IMG_SIZE = (2030, 1354)\n",
    "\n",
    "def image_to_patches(image):\n",
    "    height, width = INT_IMG_SIZE\n",
    "\n",
    "    image_resized = tf.squeeze(tf.image.resize(image, INT_IMG_SIZE))\n",
    "    image_reshaped = tf.reshape(image_resized, [height // IMG_SIZE, IMG_SIZE, -1, IMG_SIZE, 3])\n",
    "    image_transposed = tf.transpose(image_reshaped, [0, 2, 1, 3, 4])\n",
    "    return tf.reshape(image_transposed, [-1, IMG_SIZE, IMG_SIZE, 3])\n",
    "\n",
    "\n",
    "def patches_to_image(patches):\n",
    "    height, width = INT_IMG_SIZE\n",
    "\n",
    "    image_reshaped = tf.reshape(tf.squeeze(patches), [-1, height // IMG_SIZE, width // IMG_SIZE, IMG_SIZE, IMG_SIZE])\n",
    "    image_transposed = tf.transpose(image_reshaped, [0, 1, 3, 2, 4])\n",
    "    image_resized = tf.reshape(image_transposed, [-1, height, width])\n",
    "    return tf.squeeze(tf.image.resize(tf.expand_dims(image_resized, axis=-1), ORIG_IMG_SIZE), axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d258f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_patching(test_image=1):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(x_train[test_image])\n",
    "    plt.show()\n",
    "    patches = image_to_patches(x_train[test_image])[..., 0]\n",
    "    plt.imshow(patches[0])\n",
    "    plt.show()\n",
    "    plt.imshow(patches_to_image(patches)[0, ...])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e075f81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, None, 3)]   0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "tf.image.resize (TFOpLambda) (None, 2240, 1344, 3)     0         \n",
      "_________________________________________________________________\n",
      "tf.compat.v1.squeeze (TFOpLa None                      0         \n",
      "_________________________________________________________________\n",
      "tf.reshape (TFOpLambda)      (5, 448, None, 448, 3)    0         \n",
      "_________________________________________________________________\n",
      "tf.compat.v1.transpose (TFOp (5, None, 448, 448, 3)    0         \n",
      "_________________________________________________________________\n",
      "tf.reshape_1 (TFOpLambda)    (None, 448, 448, 3)       0         \n",
      "_________________________________________________________________\n",
      "model_1 (Functional)         (None, None, None, 1)     67295194  \n",
      "_________________________________________________________________\n",
      "tf.compat.v1.squeeze_1 (TFOp None                      0         \n",
      "_________________________________________________________________\n",
      "tf.reshape_2 (TFOpLambda)    (None, 5, 3, 448, 448)    0         \n",
      "_________________________________________________________________\n",
      "tf.compat.v1.transpose_1 (TF (None, 5, 448, 3, 448)    0         \n",
      "_________________________________________________________________\n",
      "tf.reshape_3 (TFOpLambda)    (None, 2240, 1344)        0         \n",
      "_________________________________________________________________\n",
      "tf.expand_dims (TFOpLambda)  (None, 2240, 1344, 1)     0         \n",
      "_________________________________________________________________\n",
      "tf.image.resize_1 (TFOpLambd (None, 2030, 1354, 1)     0         \n",
      "_________________________________________________________________\n",
      "tf.compat.v1.squeeze_2 (TFOp (None, 2030, 1354)        0         \n",
      "=================================================================\n",
      "Total params: 67,295,194\n",
      "Trainable params: 67,149,332\n",
      "Non-trainable params: 145,862\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "# Automatically mirror training across all available GPUs\n",
    "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "with strategy.scope():\n",
    "\n",
    "    base_model = Unet(backbone, encoder_weights='imagenet', encoder_freeze=encoder_freeze,\n",
    "                 classes=1, activation='sigmoid')\n",
    "\n",
    "    inp = Input(shape=(None, None, 3))\n",
    "    rescaled = rescale(inp)\n",
    "    tiled = image_to_patches(rescaled)\n",
    "    # tiled = tile(resized)\n",
    "    # augmented = data_augmentation(tiled)\n",
    "    mod = base_model(tiled)\n",
    "    out = patches_to_image(mod)\n",
    "    # I have to use this rather than tf.image.resize because by this point I've lost the channel information\n",
    "    # out = tf.keras.Sequential([layers.experimental.preprocessing.Resizing(*ORIG_IMG_SIZE)])(repatched)\n",
    "\n",
    "    model = Model(inp, out, name=base_model.name)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(Adam(learning_rate=learning_rate), loss=losses[loss], metrics=[iou_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d4e7eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard, ReduceLROnPlateau\n",
    "tensorboard = TensorBoard(log_dir=tensorboard_dir, histogram_freq=0,\n",
    "                          write_images=True, write_graph=False)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=5e-7, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db3768e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_data_flow(data, labels, subset, batch_size=1):\n",
    "#     # this is the augmentation configuration we will use for training\n",
    "#     from keras.preprocessing.image import ImageDataGenerator\n",
    "#     datagen = ImageDataGenerator(\n",
    "#         shear_range=0.2,\n",
    "#         zoom_range=0.2,\n",
    "#         horizontal_flip=True,\n",
    "#         validation_split=0.2)\n",
    "#     generator = datagen.flow(\n",
    "#         data, y=labels,\n",
    "#         batch_size=batch_size if subset == 'training' else 1,\n",
    "#         subset=subset)\n",
    "#     return generator\n",
    "\n",
    "def get_data_flow(data, labels, subset, batch_size=1):\n",
    "    # we create two instances with the same arguments\n",
    "    data_gen_args = dict(\n",
    "                         shear_range=0.2,\n",
    "                         zoom_range=0.2,\n",
    "                         horizontal_flip=True,\n",
    "                        )\n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    # Provide the same seed and keyword arguments to the fit and flow methods\n",
    "    seed = 1\n",
    "    image_datagen.fit(data, augment=True, seed=seed)\n",
    "    mask_datagen.fit(labels, augment=True, seed=seed)\n",
    "    image_generator = image_datagen.flow(\n",
    "        data,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        seed=seed)\n",
    "    mask_generator = mask_datagen.flow(\n",
    "        labels,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        seed=seed)\n",
    "    # combine generators into one which yields image and masks\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    return train_generator\n",
    "\n",
    "if augment:\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "473b425f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "INFO:tensorflow:batch_all_reduce: 492 all-reduces with algorithm = hierarchical_copy, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 492 all-reduces with algorithm = hierarchical_copy, num_packs = 1\n",
      "858/858 [==============================] - 813s 823ms/step - loss: nan - iou_score: 0.0018 - val_loss: 1.0138 - val_iou_score: 9.5331e-04\n",
      "Epoch 2/30\n",
      "858/858 [==============================] - 688s 800ms/step - loss: nan - iou_score: 0.0026 - val_loss: 1.0195 - val_iou_score: 0.0017\n",
      "Epoch 3/30\n",
      "858/858 [==============================] - 689s 802ms/step - loss: nan - iou_score: 0.0108 - val_loss: 1.0313 - val_iou_score: 2.0574e-07\n",
      "Epoch 4/30\n",
      "858/858 [==============================] - 691s 803ms/step - loss: nan - iou_score: 0.0323 - val_loss: 1.0488 - val_iou_score: 0.0039\n",
      "Epoch 5/30\n",
      "858/858 [==============================] - 688s 801ms/step - loss: nan - iou_score: 0.0525 - val_loss: 1.0493 - val_iou_score: 0.0046\n",
      "Epoch 6/30\n",
      "858/858 [==============================] - 687s 799ms/step - loss: nan - iou_score: 0.0665 - val_loss: 1.0621 - val_iou_score: 0.0043\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "Epoch 7/30\n",
      "858/858 [==============================] - 687s 799ms/step - loss: nan - iou_score: 0.1041 - val_loss: 1.0246 - val_iou_score: 0.0037\n",
      "Epoch 8/30\n",
      "858/858 [==============================] - 686s 798ms/step - loss: nan - iou_score: 0.1226 - val_loss: 1.0241 - val_iou_score: 0.0029\n",
      "Epoch 9/30\n",
      "858/858 [==============================] - 686s 798ms/step - loss: nan - iou_score: 0.1298 - val_loss: 1.0272 - val_iou_score: 0.0034\n",
      "Epoch 10/30\n",
      "858/858 [==============================] - 685s 796ms/step - loss: nan - iou_score: 0.1438 - val_loss: 1.0250 - val_iou_score: 0.0026\n",
      "Epoch 11/30\n",
      "858/858 [==============================] - 685s 797ms/step - loss: nan - iou_score: 0.1532 - val_loss: 1.0262 - val_iou_score: 0.0030\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "Epoch 12/30\n",
      "858/858 [==============================] - 684s 796ms/step - loss: nan - iou_score: 0.1646 - val_loss: 1.0271 - val_iou_score: 0.0039\n",
      "Epoch 13/30\n",
      "858/858 [==============================] - 684s 795ms/step - loss: nan - iou_score: 0.1724 - val_loss: 1.0275 - val_iou_score: 0.0038\n",
      "Epoch 14/30\n",
      "858/858 [==============================] - 684s 796ms/step - loss: nan - iou_score: 0.1786 - val_loss: 1.0283 - val_iou_score: 0.0045\n",
      "Epoch 15/30\n",
      "858/858 [==============================] - 684s 795ms/step - loss: nan - iou_score: 0.1838 - val_loss: 1.0291 - val_iou_score: 0.0046\n",
      "Epoch 16/30\n",
      "858/858 [==============================] - 685s 797ms/step - loss: nan - iou_score: 0.1856 - val_loss: 1.0283 - val_iou_score: 0.0045\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "Epoch 17/30\n",
      "858/858 [==============================] - 685s 797ms/step - loss: nan - iou_score: 0.1910 - val_loss: 1.0280 - val_iou_score: 0.0045\n",
      "Epoch 18/30\n",
      "858/858 [==============================] - 685s 797ms/step - loss: nan - iou_score: 0.1956 - val_loss: 1.0283 - val_iou_score: 0.0044\n",
      "Epoch 19/30\n",
      "858/858 [==============================] - 683s 795ms/step - loss: nan - iou_score: 0.1946 - val_loss: 1.0283 - val_iou_score: 0.0044\n",
      "Epoch 20/30\n",
      "858/858 [==============================] - 684s 795ms/step - loss: nan - iou_score: 0.1914 - val_loss: 1.0281 - val_iou_score: 0.0044\n",
      "Epoch 21/30\n",
      "858/858 [==============================] - 685s 796ms/step - loss: nan - iou_score: 0.1943 - val_loss: 1.0285 - val_iou_score: 0.0045\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "Epoch 22/30\n",
      "858/858 [==============================] - 685s 797ms/step - loss: nan - iou_score: 0.1972 - val_loss: 1.0285 - val_iou_score: 0.0046\n",
      "Epoch 23/30\n",
      "858/858 [==============================] - 684s 795ms/step - loss: nan - iou_score: 0.1965 - val_loss: 1.0285 - val_iou_score: 0.0046\n",
      "Epoch 24/30\n",
      "858/858 [==============================] - 686s 798ms/step - loss: nan - iou_score: 0.1980 - val_loss: 1.0284 - val_iou_score: 0.0046\n",
      "Epoch 25/30\n",
      "858/858 [==============================] - 685s 796ms/step - loss: nan - iou_score: 0.1957 - val_loss: 1.0285 - val_iou_score: 0.0046\n",
      "Epoch 26/30\n",
      "858/858 [==============================] - 684s 796ms/step - loss: nan - iou_score: 0.1984 - val_loss: 1.0285 - val_iou_score: 0.0045\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "Epoch 27/30\n",
      "858/858 [==============================] - 684s 796ms/step - loss: nan - iou_score: 0.1966 - val_loss: 1.0285 - val_iou_score: 0.0046\n",
      "Epoch 28/30\n",
      "858/858 [==============================] - 685s 797ms/step - loss: nan - iou_score: 0.2004 - val_loss: 1.0286 - val_iou_score: 0.0045\n",
      "Epoch 29/30\n",
      "858/858 [==============================] - 685s 797ms/step - loss: nan - iou_score: 0.1964 - val_loss: 1.0284 - val_iou_score: 0.0045\n",
      "Epoch 30/30\n",
      "858/858 [==============================] - 685s 797ms/step - loss: nan - iou_score: 0.2012 - val_loss: 1.0286 - val_iou_score: 0.0045\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset.shuffle(100).batch(2).prefetch(tf.data.AUTOTUNE), validation_data=val_dataset.batch(4).prefetch(tf.data.AUTOTUNE), verbose=1,\n",
    "                    epochs=epochs, callbacks=[tensorboard, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d4323a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try augmentation\n",
    "# Try shuffling the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ebd5045",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-10748484d581>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss    :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_ds' is not defined"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_ds, verbose=0)\n",
    "\n",
    "print('Test loss    :', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6824da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save Keras model for Tensorflow Serving\n",
    "tf.saved_model.save(\n",
    "    model,\n",
    "    os.path.join(model_dir, 'model/1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20303451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
